{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep deterministic policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, action_dims=1):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l0 = tf.keras.layers.Dense(64, activation=tf.nn.relu)\n",
    "        self.l1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.l2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "        self.l3 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "        \n",
    "        self.mu = tf.keras.layers.Dense(action_dims, activation=tf.nn.tanh)\n",
    "        self.sigma = tf.keras.layers.Dense(action_dims, activation=tf.nn.softplus)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.l0(inputs)\n",
    "        h = self.l1(h)\n",
    "        h = self.l2(h)\n",
    "        h = self.l3(h)   \n",
    "        \n",
    "        mu = self.mu(h)\n",
    "        sigma = self.sigma(h)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## critic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, ):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.l0 = tf.keras.layers.Dense(64, activation=tf.nn.relu)\n",
    "        self.l1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.l2 = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "        self.l3 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "        self.l4 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs = tf.keras.layers.concatenate(inputs, axis=-1)\n",
    "        h = self.l0(inputs)\n",
    "        h = self.l1(h)\n",
    "        h = self.l2(h)                \n",
    "        h = self.l3(h)                \n",
    "        q = self.l4(h) \n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, mem_size=(1024, 9)):\n",
    "        self.mem = np.zeros(shape=mem_size, dtype=np.float32)\n",
    "        \n",
    "        self.pointer = 0\n",
    "        self.mem_size = mem_size\n",
    "        self.full = False\n",
    "    \n",
    "    def add(self, obs, a, r, done, obs_):\n",
    "        exp = np.concatenate((obs, a, [r, done], obs_), axis=-1).astype(np.float32)\n",
    "        \n",
    "        if self.pointer >= self.mem_size[0]:\n",
    "            self.pointer = 0\n",
    "            self.full = True\n",
    "            \n",
    "        self.mem[self.pointer] = exp\n",
    "        \n",
    "        self.pointer += 1\n",
    "        \n",
    "        return self.full\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        idx = np.random.randint(self.mem_size[0], size=batch_size)\n",
    "        return self.mem[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, mem_size=(1024,9), action_dims=1, obs_dims=3):\n",
    "        self.mem = Memory(mem_size)\n",
    "        \n",
    "        self.actor = Actor(action_dims=1)\n",
    "        self.actor_target = Actor(action_dims=1)\n",
    "        \n",
    "        self.critic = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        \n",
    "        '''\n",
    "        self.actor.build(input_shape=(None, obs_dims))\n",
    "        self.actor_target.build(input_shape=(None, obs_dims))\n",
    "        self.actor_target.set_weights(self.actor.get_weights())\n",
    "        \n",
    "        self.critic.build(input_shape=(None, obs_dims+action_dims))\n",
    "        self.critic_target.build(input_shape=(None, obs_dims+action_dims))\n",
    "        self.critic_target.set_weights(self.critic.get_weights())\n",
    "        '''\n",
    "        \n",
    "        self.gamma = 0.9\n",
    "        self.actor_lr = 1e-3\n",
    "        self.critic_lr = 1e-3\n",
    "        self.critic_target_update_time = 10\n",
    "        self.actor_target_update_time = 20\n",
    "        self.tau = 0.8\n",
    "        \n",
    "        self.learning_times = 0\n",
    "        self.obs_dims = obs_dims\n",
    "        self.action_dims = action_dims\n",
    "        self.status = {\"buffer_full\": False,\n",
    "                       \"learning_times\": 0,\n",
    "                       \"update_actor\": 0,\n",
    "                       \"update_critic\": 0,\n",
    "                       \"reward_mean\": 0,\n",
    "                       \"loss_critic\": None,\n",
    "                       \"loss_actor\": None,\n",
    "                      }\n",
    "    \n",
    "    def get_action(self, obs, train=False):\n",
    "        obs = tf.expand_dims(obs, axis=0)\n",
    "        obs = tf.cast(obs, dtype=tf.float32)\n",
    "        mu, sigma = self.actor(obs)\n",
    "        disb = tfp.distributions.Normal(loc=mu*2., scale=sigma + 0.1 + 1. * float(train))\n",
    "        a = disb.sample(1)[0,0]\n",
    "        a = tf.clip_by_value(a, -2., 2.)\n",
    "        return a\n",
    "    \n",
    "    def add_exp(self, obs, a, r, done, obs_):\n",
    "        full = self.mem.add(obs, a, r, done, obs_)\n",
    "        self.status[\"buffer_full\"] = full\n",
    "        return full\n",
    "    \n",
    "    def learning(self, batch_size):\n",
    "        mini_batch = self.mem.get_batch(batch_size)\n",
    "        \n",
    "        obs = mini_batch[:,0:self.obs_dims]\n",
    "        a = np.expand_dims(mini_batch[:, self.obs_dims], axis=-1)\n",
    "        r = np.expand_dims(mini_batch[:, self.obs_dims+1], axis=-1)\n",
    "        done = np.expand_dims(mini_batch[:, self.obs_dims+2], axis=-1)\n",
    "        obs_ = mini_batch[:, -self.obs_dims:]\n",
    "        \n",
    "        mu_, sigma_ = self.actor_target(obs_)\n",
    "        disb = tfp.distributions.Normal(loc=mu_*2., scale=sigma_)\n",
    "        a_ = disb.sample(1)[0]\n",
    "        q_ = self.critic_target([obs_, a_])\n",
    "        \n",
    "        with tf.GradientTape() as tape_c:\n",
    "            q = self.critic([obs, a])\n",
    "            q_target = r + self.gamma * q_ * (1. - done)\n",
    "        \n",
    "            loss_critic = tf.reduce_mean(tf.losses.mse(q_target, q))\n",
    "\n",
    "        grads_critic = tape_c.gradient(target=loss_critic, sources=self.critic.trainable_variables)\n",
    "        tf.optimizers.Adam(self.critic_lr).apply_gradients(zip(grads_critic, self.critic.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as tape_a:\n",
    "            mu, sigma = self.actor(obs)\n",
    "            distrib = tfp.distributions.Normal(loc=mu*2., scale=sigma)\n",
    "            a_pred = distrib.sample(1)[0]\n",
    "            a_pred = tf.clip_by_value(a_pred, -2., 2.)\n",
    "            q = self.critic([obs, a_pred])\n",
    "            \n",
    "            loss_actor = -tf.reduce_mean(q)\n",
    "        \n",
    "        grads_actor = tape_a.gradient(target=loss_actor, sources=self.actor.trainable_variables)\n",
    "        tf.optimizers.Adam(self.actor_lr).apply_gradients(zip(grads_actor, self.actor.trainable_variables))\n",
    "        \n",
    "        self.__update_critic_target()\n",
    "        self.__update_actor_target()\n",
    "        \n",
    "        self.learning_times += 1        \n",
    "        self.status[\"learning_times\"] = self.learning_times\n",
    "        self.status[\"loss_critic\"] = float(loss_critic)\n",
    "        self.status[\"loss_actor\"] = float(loss_actor)\n",
    "    \n",
    "    def __update_critic_target(self, ):\n",
    "        if self.learning_times % self.critic_target_update_time == 0:\n",
    "            weights = []\n",
    "            for w_target, w in zip(self.critic_target.get_weights(), self.critic.get_weights()):\n",
    "                weights.append(self.tau * w_target + (1.-self.tau) * w)\n",
    "            self.critic_target.set_weights(weights)\n",
    "            self.status[\"update_critic\"] += 1\n",
    "    \n",
    "    def __update_actor_target(self, ):\n",
    "        if self.learning_times % self.actor_target_update_time == 0:\n",
    "            weights = []\n",
    "            for w_target, w in zip(self.actor_target.get_weights(), self.actor.get_weights()):\n",
    "                weights.append(self.tau * w_target + (1.-self.tau) * w)\n",
    "            self.actor_target.set_weights(weights)\n",
    "            self.status[\"update_actor\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test in env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "agent = DDPG(mem_size=(1024, 9), action_dims=1, obs_dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_len = 200\n",
    "learn_per_step = 8\n",
    "test_per_episode = 5\n",
    "test_episodes = 5\n",
    "\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    for _ in range(episode_len):\n",
    "        a = agent.get_action(obs, train=True)\n",
    "        obs_, r, _, info = env.step(a)\n",
    "        \n",
    "        full = agent.add_exp(obs, a, r, False, obs_)\n",
    "        \n",
    "        if full and step % learn_per_step == 0:\n",
    "            agent.learning(batch_size=256)\n",
    "            print(agent.status)\n",
    "        \n",
    "        obs = obs_\n",
    "            \n",
    "        step += 1\n",
    "    \n",
    "    if episode % test_per_episode == 0:\n",
    "        print(\"---- start testing...\")\n",
    "        total_reward = 0\n",
    "        for _ in range(test_episodes):\n",
    "            s = env.reset()\n",
    "            for _ in range(episode_len):\n",
    "                act = agent.get_action(s)\n",
    "                s_, rd, _, info = env.step(act)\n",
    "                env.render()\n",
    "                s = s_\n",
    "                total_reward += rd\n",
    "        agent.status[\"reward_mean\"] = float(total_reward / test_episodes)\n",
    "    \n",
    "    episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
