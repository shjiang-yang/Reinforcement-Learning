{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNQ algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, buffer_size=[1024,7]):\n",
    "        self.row_cap, self.col_cap = buffer_size\n",
    "        self.buffer = np.zeros(shape=(self.row_cap, self.col_cap), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def add_exp(self, obs, action, reward, obs_, done):\n",
    "        exp = np.hstack((obs, action, reward, obs_, done)).astype(np.float32)\n",
    "        idx = self.pointer % self.row_cap\n",
    "        self.buffer[idx, :] = exp\n",
    "        buffer_ready = False if self.pointer < self.row_cap else True\n",
    "        \n",
    "        self.pointer += 1\n",
    "        return buffer_ready\n",
    "    \n",
    "    def get_minibatch(self, batch_size):\n",
    "        idx = np.random.randint(self.row_cap, size=batch_size)\n",
    "        return self.buffer[idx, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_model(tf.keras.Model):\n",
    "    def __init__(self, output_layer_dim):\n",
    "        super(Net_model, self).__init__()\n",
    "        \n",
    "        self.layer0 = tf.keras.layers.Dense(units=128)\n",
    "        self.layer1 = tf.keras.layers.Dense(units=64)\n",
    "        self.layer2 = tf.keras.layers.Dense(units=output_layer_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h0 = self.layer0(inputs)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        h1 = self.layer1(h0)\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        \n",
    "        out = self.layer2(h1)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, action_n, observation_dim, buffer_size=[1024,7]):\n",
    "        self.replay_buffer = Replay_buffer(buffer_size)\n",
    "        self.eval_net = Net_model(action_n)\n",
    "        self.target_net = Net_model(action_n)\n",
    "        \n",
    "        # super params\n",
    "        self.gamma = 0.8\n",
    "        self.epsilon = 1.0\n",
    "        self.eps_decrs = 1e-6\n",
    "        self.min_eps = 0.05\n",
    "        self.lr = 1e-3\n",
    "        self.batch_size = int(buffer_size[0] / 32)\n",
    "        self.obs_dim = observation_dim\n",
    "        self.action_n = action_n\n",
    "        self.learning_times = 0\n",
    "        self.sync_weights_per_times = 100\n",
    "        self.sync_weights_times = 0\n",
    "        \n",
    "        self.agent_status = {\"buffer_ready\": False,\n",
    "                             \"learn_times\": self.learning_times,\n",
    "                             \"sync_times\": self.sync_weights_times,\n",
    "                             \"loss\": None,\n",
    "                             \"eps_cur\": None}\n",
    "    \n",
    "    def get_action(self, obs, eps_greedy=True):\n",
    "        eps = max(self.epsilon - self.eps_decrs * self.learning_times, self.min_eps)\n",
    "        self.agent_status[\"eps_cur\"] = eps\n",
    "        if np.random.uniform() < eps and eps_greedy:\n",
    "            action = np.random.randint(self.action_n)\n",
    "        else:\n",
    "            obs = tf.constant(obs, dtype=tf.float32)\n",
    "            obs = tf.expand_dims(obs, axis=0)\n",
    "            action = tf.argmax(self.eval_net(obs)[0])\n",
    "        return int(action)\n",
    "    \n",
    "    def add_exp(self, obs, action, reward, obs_, done):\n",
    "        ready = self.replay_buffer.add_exp(obs, action, reward, obs_, done)\n",
    "        self.agent_status[\"buffer_ready\"] = ready\n",
    "        return ready\n",
    "    \n",
    "    def learning(self, ):\n",
    "        minibatch = self.replay_buffer.get_minibatch(self.batch_size)\n",
    "        obs = minibatch[:, 0:self.obs_dim]\n",
    "        action = minibatch[:, self.obs_dim]\n",
    "        reward = minibatch[:, self.obs_dim+1]\n",
    "        obs_ = minibatch[:, -self.obs_dim-1:-1]\n",
    "        done = minibatch[:, -1]\n",
    "        \n",
    "        q_target = reward + self.gamma * tf.reduce_max(self.target_net(obs_), axis=1) * (1 - done)\n",
    "        \n",
    "        action = tf.cast(action, dtype=tf.int32)\n",
    "        action_onehot = tf.one_hot(action, depth=self.action_n, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_value = self.eval_net(obs)\n",
    "            q_value = q_value * action_onehot\n",
    "            q_value = tf.reduce_sum(q_value, axis=1)\n",
    "            \n",
    "            loss = tf.losses.mean_squared_error(q_target, q_value)\n",
    "            \n",
    "        grads = tape.gradient(target=loss, sources=self.eval_net.trainable_variables)\n",
    "        tf.optimizers.Adam(self.lr).apply_gradients(zip(grads, self.eval_net.trainable_variables))\n",
    "        \n",
    "        self.agent_status[\"learn_times\"] = self.learning_times\n",
    "        self.agent_status[\"loss\"] = float(loss)\n",
    "        \n",
    "        if self.learning_times % self.sync_weights_per_times == 0:\n",
    "            self.__sync_weights_to_target()\n",
    "        \n",
    "        self.learning_times += 1\n",
    "               \n",
    "    def __sync_weights_to_target(self, ):\n",
    "        self.target_net.set_weights(self.eval_net.get_weights())\n",
    "        self.agent_status[\"sync_times\"] = self.sync_weights_times\n",
    "        self.sync_weights_times += 1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "action_n = env.action_space.n \n",
    "obs_dim = env.observation_space.shape[0]\n",
    "agent = DQN(action_n=action_n, observation_dim=obs_dim, buffer_size=[2048, obs_dim*2+3])\n",
    "\n",
    "step = 0\n",
    "learning_per_step = 1\n",
    "episode = 0\n",
    "evaluate_per_episode = 10\n",
    "evaluate_times = 10\n",
    "\n",
    "while True:\n",
    "    # train\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = agent.get_action(obs)\n",
    "        obs_, reward, done, _ = env.step(a)\n",
    "        ready = agent.add_exp(obs, a, reward, obs_, done)\n",
    "        obs = obs_\n",
    "        \n",
    "        if ready and step % learning_per_step == 0:\n",
    "            agent.learning()\n",
    "        step += 1\n",
    "        \n",
    "        print(\"\\r** agent status: {}\".format(agent.agent_status), end='', flush=True)\n",
    "    \n",
    "    episode += 1\n",
    "    \n",
    "    # evaluate\n",
    "    if ready and episode % evaluate_per_episode == 0:\n",
    "        print(\"\\n-------------------- evaluating -----------------------\\n\")\n",
    "        total_reward = 0\n",
    "        for _ in range(evaluate_times):\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                a = agent.get_action(obs, eps_greedy=False)\n",
    "                env.render()\n",
    "                obs_, reward, done, _ = env.step(a)\n",
    "                total_reward += reward\n",
    "                obs = obs_\n",
    "        agent.agent_status[\"reward\"] = total_reward / evaluate_times\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
