{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, units=[32, 16, 8, 1]):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.l0 = tf.keras.layers.Dense(units=units[0], activation=tf.nn.relu)\n",
    "        self.l1 = tf.keras.layers.Dense(units=units[1], activation=tf.nn.relu)\n",
    "        self.l2 = tf.keras.layers.Dense(units=units[2], activation=tf.nn.relu)\n",
    "        self.l3 = tf.keras.layers.Dense(units=units[3], activation=None)\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.l0(inputs)\n",
    "        h = self.l1(h)\n",
    "        h = self.l2(h)\n",
    "        out = self.l3(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor network  \n",
    "continous action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, units=[32, 16, 8], action_dims=1):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l0 = tf.keras.layers.Dense(units=units[0], activation=tf.nn.relu)\n",
    "        self.l1 = tf.keras.layers.Dense(units=units[1], activation=tf.nn.relu)\n",
    "        self.l2 = tf.keras.layers.Dense(units=units[2], activation=tf.nn.relu)\n",
    "        \n",
    "        self.mu = tf.keras.layers.Dense(units=action_dims, activation=tf.nn.tanh)\n",
    "        self.sigma = tf.keras.layers.Dense(units=action_dims, activation=tf.nn.softplus)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        h = self.l0(inputs)\n",
    "        h = self.l1(h)\n",
    "        h = self.l2(h)\n",
    "        \n",
    "        mu = self.mu(h) * 2.\n",
    "        sigma = self.sigma(h)\n",
    "        return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, obs_dims, act_dims):\n",
    "        self.gamma = 0.9\n",
    "        self.critic_lr = 2e-4 \n",
    "        self.actor_lr = 1e-4\n",
    "        self.clip_epsilon = 0.2\n",
    "        self.sync_critic_steps = 10\n",
    "        self.obs_dims = obs_dims\n",
    "        self.act_dims = act_dims\n",
    "        \n",
    "        self.sync_critic_cnt = 0\n",
    "        \n",
    "        self.critic_new = Critic(units=[32, 16, 8, 1])\n",
    "        self.critic_old = Critic(units=[32, 16, 8, 1])\n",
    "        self.critic_new.build(input_shape=(None, self.obs_dims))\n",
    "        self.critic_old.build(input_shape=(None, self.obs_dims))\n",
    "        self.critic_old.set_weights(self.critic_new.get_weights())\n",
    "        \n",
    "        self.actor_new = Actor(units=[32, 16, 8], action_dims=self.act_dims)\n",
    "        self.actor_old = Actor(units=[32, 16, 8], action_dims=self.act_dims)\n",
    "        self.actor_new.build(input_shape=(None, self.obs_dims))\n",
    "        self.actor_old.build(input_shape=(None, self.obs_dims))\n",
    "        self.actor_old.set_weights(self.actor_new.get_weights())\n",
    "        \n",
    "        self.status = {\"episode\": None,\n",
    "                       \"critic_loss\": None,\n",
    "                       \"actor_obj\": None,\n",
    "                       \"critic_sync_times\": 0,\n",
    "                       \"actor_sync_times\": 0\n",
    "                       }\n",
    "        \n",
    "    \n",
    "    def choose_action(self, obs):\n",
    "        mu, sigma = self.actor_old(obs)\n",
    "        distb = tfp.distributions.Normal(loc=mu[0], scale=sigma[0])\n",
    "        action = distb.sample(1)[0]\n",
    "        return action.numpy()\n",
    "    \n",
    "    def critic_training(self, obs, q_target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_value = self.critic_new(obs)\n",
    "            loss = tf.reduce_mean(tf.losses.mean_squared_error(q_target, q_value))\n",
    "        \n",
    "        grads = tape.gradient(target=loss, sources=self.critic_new.trainable_variables)\n",
    "        tf.optimizers.Adam(self.critic_lr).apply_gradients(zip(grads, self.critic_new.trainable_variables))\n",
    "        \n",
    "        self.sync_critic_cnt += 1\n",
    "        if self.sync_critic_cnt % self.sync_critic_steps == 0:\n",
    "            self.__sync_critic_params()\n",
    "        \n",
    "        self.status[\"critic_loss\"] = float(loss)\n",
    "    \n",
    "    def actor_training(self, obs, actions, q_target):\n",
    "        advantage = q_target - self.critic_new(obs)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            mu_new, sigma_new = self.actor_new(obs)\n",
    "            mu_old, sigma_old = self.actor_old(obs)\n",
    "            \n",
    "            distb_new = tfp.distributions.Normal(loc=mu_new, scale=sigma_new)\n",
    "            distb_old = tfp.distributions.Normal(loc=mu_old, scale=sigma_old)\n",
    "            \n",
    "            prob_new = distb_new.prob(actions)\n",
    "            prob_old = distb_old.prob(actions)\n",
    "            \n",
    "            ratio = prob_new / (prob_old + 1e-8)\n",
    "            \n",
    "            objective = - tf.reduce_mean(\n",
    "                tf.minimum(ratio * advantage,\n",
    "                           tf.clip_by_value(ratio, 1.-self.clip_epsilon, 1.+self.clip_epsilon) * advantage)\n",
    "            )\n",
    "        \n",
    "        grads = tape.gradient(target=objective, sources=self.actor_new.trainable_variables)\n",
    "        tf.optimizers.Adam(self.actor_lr).apply_gradients(zip(grads, self.actor_new.trainable_variables))\n",
    "            \n",
    "        self.status[\"actor_obj\"] = float(objective)\n",
    "    \n",
    "    def __sync_critic_params(self, ):\n",
    "        self.critic_old.set_weights(self.critic_new.get_weights())\n",
    "        self.status[\"critic_sync_times\"] += 1\n",
    "    \n",
    "    def sync_actor_params(self, ):\n",
    "        self.actor_old.set_weights(self.actor_new.get_weights())\n",
    "        self.status[\"actor_sync_times\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "agent = PPO(obs_dims=env.observation_space.shape[0], act_dims=env.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_step = 256\n",
    "batch_size = 32\n",
    "learning_times = 15\n",
    "test_per_episode = 5\n",
    "test_episode = 5\n",
    "gamma = 0.9\n",
    "\n",
    "episode = 0\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    obs_seq, reward_seq, action_seq = [], [], []\n",
    "    for step in range(max_step):\n",
    "        print(agent.actor_old(np.array([[1.,0.,0.]]).astype(np.float32)))\n",
    "        print(\"[1.,0.,0.] action: \", agent.choose_action(np.array([[1.,0.,0.]]).astype(np.float32)))\n",
    "        print(agent.actor_old(np.array([[1.,0.,0.5]]).astype(np.float32)))\n",
    "        print(\"[1.,0.,0.5] action: \", agent.choose_action(np.array([[1.,0.,0.5]]).astype(np.float32)))\n",
    "        action = agent.choose_action(np.array([obs]).astype(np.float32))\n",
    "        action = np.clip(action, -2., 2.)\n",
    "        obs_, reward, done, info = env.step(action)\n",
    "        reward = (reward + 8.0)\n",
    "        \n",
    "        if len(obs_seq) < batch_size-1:\n",
    "            obs_seq.append(obs)\n",
    "            reward_seq.append(reward)\n",
    "            action_seq.append(action)\n",
    "\n",
    "        else:   \n",
    "            obs_seq.append(obs)\n",
    "            reward_seq.append(reward)\n",
    "            action_seq.append(action)\n",
    "            \n",
    "            q_target = [agent.critic_old(np.array([obs_]).astype(np.float32)).numpy()[0]]\n",
    "            for r in reward_seq[::-1]:\n",
    "                q_target.append(r + gamma * q_target[-1])\n",
    "            q_target.pop(0)\n",
    "            q_target.reverse()\n",
    "            \n",
    "            obs_buf = np.vstack(obs_seq).astype(np.float32)\n",
    "            action_buf = np.vstack(action_seq).astype(np.float32)\n",
    "            q_target = np.vstack(q_target).astype(np.float32)\n",
    "            \n",
    "            for _ in range(learning_times):\n",
    "                agent.critic_training(obs=obs_buf, q_target=q_target)\n",
    "                print(agent.status)\n",
    "                \n",
    "            for _ in range(learning_times):\n",
    "                agent.actor_training(obs=obs_buf, actions=action_buf, q_target=q_target)\n",
    "                print(agent.status)\n",
    "            \n",
    "            obs_seq, reward_seq, action_seq = [], [], []\n",
    "            agent.sync_actor_params()\n",
    "        \n",
    "        obs = obs_\n",
    "        \n",
    "    agent.status[\"episode\"] = episode\n",
    "    episode += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    if episode % test_per_episode == 0:\n",
    "        for ep in range(test_episode):\n",
    "            toltal_reward = 0\n",
    "            s = env.reset()\n",
    "            for step in range(max_step):\n",
    "                a = agent.choose_action(np.array([s]).astype(np.float32))\n",
    "                a = np.clip(a, -2., 2.)\n",
    "                s_, r, _, _ = env.step(a)\n",
    "                env.render()\n",
    "                toltal_reward += r\n",
    "                s = s_\n",
    "                print(\"obs:{}, action:{}, reward:{}\".format(s, a, r))\n",
    "        print(\"********* episode: {}, toltal_reward: {}\".format(episode, toltal_reward/test_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
